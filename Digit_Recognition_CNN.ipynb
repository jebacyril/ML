{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Programming Task: Digit recognition using CNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data as Data\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torchvision import datasets, transforms\n",
    "from torchinfo import summary\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i. Complete the code for the ConvNet class given below using the network description from supplement pdf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "        \n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.fc1 = nn.Conv2d(in_channels=1, out_channels=20, kernel_size=5, stride=1)\n",
    "        self.fc2 = nn.MaxPool2d(kernel_size=2)\n",
    "        self.fc3 = nn.Linear(in_features=20*12*12, out_features=100)\n",
    "        # [(28 + 2*0 - 5)/1] + 1 = 24\n",
    "        # Since we applied maxpooling with kernel_size=2 and stride=2\n",
    "        # (24 -2 / 2) + 1 = 12\n",
    "        # output_channel=20 of conv layer\n",
    "        self.fc4 = nn.Linear(in_features=100, out_features=10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x) \n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return F.log_softmax(x,dim=1) \n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the net."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvNet(\n",
      "  (fc1): Conv2d(1, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc3): Linear(in_features=2880, out_features=100, bias=True)\n",
      "  (fc4): Linear(in_features=100, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net = ConvNet()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ii. Train the CNN and observe the difference in performance in comparison to the feed-forward\n",
    "network from the task 5.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set hyper parameters.\n",
    "batch_size=200\n",
    "learning_rate=0.01\n",
    "epochs=10\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the MNIST data set.\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                   transforms.ToTensor(),\n",
    "                   transforms.Normalize((0.1307,), (0.3081,))\n",
    "                       ])),\n",
    "        batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "        ])),\n",
    "        batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the loss function and the optimization criteria\n",
    "optimiser = optim.SGD(net.parameters(), lr=0.01)\n",
    "criterion = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 Avg. Training Loss: 0.003934\n",
      "\n",
      "Train Epoch: 2 Avg. Training Loss: 0.001483\n",
      "\n",
      "Train Epoch: 3 Avg. Training Loss: 0.001185\n",
      "\n",
      "Train Epoch: 4 Avg. Training Loss: 0.000989\n",
      "\n",
      "Train Epoch: 5 Avg. Training Loss: 0.000846\n",
      "\n",
      "Train Epoch: 6 Avg. Training Loss: 0.000733\n",
      "\n",
      "Train Epoch: 7 Avg. Training Loss: 0.000647\n",
      "\n",
      "Train Epoch: 8 Avg. Training Loss: 0.000577\n",
      "\n",
      "Train Epoch: 9 Avg. Training Loss: 0.000523\n",
      "\n",
      "Train Epoch: 10 Avg. Training Loss: 0.000476\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run the main training loop\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    net.train()\n",
    "    total_loss = 0.0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        \n",
    "        optimiser.zero_grad()\n",
    "        \n",
    "        net_out = net(data)\n",
    "        \n",
    "        loss = criterion(net_out, target)\n",
    "        total_loss+=loss.data\n",
    "        \n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "\n",
    "    print('Train Epoch: {} Avg. Training Loss: {:.6f}\\n'.format(\n",
    "                epoch+1,\n",
    "                total_loss/len(train_loader.dataset)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0004, Accuracy: 9741/10000 (97.41%)\n",
      "\n",
      "Comparable accuracy compared to feed-forward network\n"
     ]
    }
   ],
   "source": [
    "test_loss = 0.0\n",
    "correct = 0.0\n",
    "\n",
    "for data, target in test_loader:    \n",
    "    net_out = net(data)\n",
    "    \n",
    "    # sum up batch loss\n",
    "    test_loss += criterion(net_out, target).data\n",
    "    \n",
    "    pred = net_out.data.max(dim=1)[1]   \n",
    "    correct += pred.eq(target.data).sum()\n",
    "\n",
    "test_loss /= len(test_loader.dataset)\n",
    "print('\\nTest set: Average loss: {:.4f}, Accuracy: {:.0f}/{} ({:.2%})\\n'.format(\n",
    "    test_loss, correct, len(test_loader.dataset),correct / len(test_loader.dataset)))\n",
    "print(\"Comparable accuracy compared to feed-forward network\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "iii. Calculate the number of learnable parameters and the output shape in each layer. Verify your\n",
    "answers with model summary. (Refer last cell of the tutorial notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "ConvNet                                  [200, 10]                 --\n",
       "├─Conv2d: 1-1                            [200, 20, 24, 24]         520\n",
       "├─MaxPool2d: 1-2                         [200, 20, 12, 12]         --\n",
       "├─Linear: 1-3                            [200, 100]                288,100\n",
       "├─Linear: 1-4                            [200, 10]                 1,010\n",
       "==========================================================================================\n",
       "Total params: 289,630\n",
       "Trainable params: 289,630\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 117.73\n",
       "==========================================================================================\n",
       "Input size (MB): 0.63\n",
       "Forward/backward pass size (MB): 18.61\n",
       "Params size (MB): 1.16\n",
       "Estimated Total Size (MB): 20.39\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(net, input_size=(200, 1, 28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 1 -- Convolutional Layer\n",
      "Output Shape\n",
      "[200, 20, 24, 24]\n",
      "Parameters\n",
      "520\n",
      "\n",
      "Layer 2 -- MaxPool Layer\n",
      "Output Shape\n",
      "[200, 20, 12, 12]\n",
      "No Parameters due to no Back propagation learning\n",
      "\n",
      "Layer 3 -- Fully Connected Layer(Linear)\n",
      "Output Shape\n",
      "[200, 100]\n",
      "Parameters\n",
      "288100\n",
      "\n",
      "Layer 4 -- Fully Connected Layer(Linear) \n",
      "Output Shape\n",
      "[200, 10]\n",
      "Parameters\n",
      "1010\n"
     ]
    }
   ],
   "source": [
    "#Layer 1 -- Convolutional Layer\n",
    "N=28 \n",
    "F=5 \n",
    "S=1\n",
    "output_channnels = 20\n",
    "Out_shape_1 = [batch_size, output_channnels, (int)(((N-F)/S) + 1), (int)(((N-F)/S) + 1)]\n",
    "print(\"Layer 1 -- Convolutional Layer\")\n",
    "print(\"Output Shape\")\n",
    "print(Out_shape_1)\n",
    "print(\"Parameters\")\n",
    "Parameters_1 = ((F*F)+1)* output_channnels\n",
    "print(Parameters_1)\n",
    "\n",
    "#Layer 2 -- MaxPool Layer\n",
    "N=24 \n",
    "F=2 \n",
    "S=2\n",
    "output_channnels = 20\n",
    "Out_shape_2 = [batch_size, 20, (int)(((N-F)/S) + 1), (int)(((N-F)/S) + 1)]\n",
    "print(\"\\nLayer 2 -- MaxPool Layer\")\n",
    "print(\"Output Shape\")\n",
    "print(Out_shape_2)\n",
    "print(\"No Parameters due to no Back propagation learning\")\n",
    "\n",
    "#Layer 3 -- Fully Connected Layer(Linear)\n",
    "# 100 Neurons\n",
    "Out_shape_3 = [batch_size, 100]\n",
    "print(\"\\nLayer 3 -- Fully Connected Layer(Linear)\")\n",
    "print(\"Output Shape\")\n",
    "print(Out_shape_3)\n",
    "print(\"Parameters\")\n",
    "input_3 = 20 * 12 * 12\n",
    "output_3 = 100\n",
    "Parameters_3 = ( input_3 * output_3 ) + 100\n",
    "print(Parameters_3)\n",
    "\n",
    "#Layer 4 -- Fully Connected Layer(Linear)\n",
    "# 10 Classes\n",
    "Out_shape_4 = [batch_size, 10]\n",
    "print(\"\\nLayer 4 -- Fully Connected Layer(Linear) \")\n",
    "print(\"Output Shape\")\n",
    "print(Out_shape_4)\n",
    "print(\"Parameters\")\n",
    "input_4 = 100\n",
    "output_4 = 10\n",
    "Parameters_4 = ( input_4 * output_4 ) + 10\n",
    "print(Parameters_4)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "7c617624a7fd88b4018bd9e75be0d58c4afb6a334791d511af9b9a5162b5af2b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
